# -*- coding: utf-8 -*-
"""Unet_Architecture_Prostate_Multiclassclassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZE22hOZe3bJvUqj2NbjLj0TWLnrhPTxc
"""

import os
import glob
import cv2
import time
import torch
import torch.nn as nn
import copy
import pandas as pd
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from copy import deepcopy
from sklearn.model_selection import train_test_split
from functools import partial
import csv
from torchvision.transforms import Compose
import torchvision.utils as vutils
from torch.optim import lr_scheduler
import numpy as np
from PIL import Image
from tqdm import tqdm
import PIL.Image as p_im
from torch import Tensor
from torch.nn.modules.loss import _Loss
from typing import List
from medpy import metric
import random
from torchvision import transforms

random.seed(42)
torch.manual_seed(42)
np.random.seed(42)

from torchinfo import summary

pip install torchinfo

pip install medpy

def get_all_patient_images(root_dir):
    all_train_images = []
    all_test_images = []
    all_val_images = []
    all_train_masks = []
    all_test_masks = []
    all_val_masks = []


    for magnification_folder in glob.glob(os.path.join(root_dir, "cropped_patches")):
        for patient_folder in glob.glob(os.path.join(magnification_folder, "patient_*_slide_01")):
            for patient__folder in glob.glob(os.path.join(patient_folder,"patient_*_slide_01")):

                img_files = glob.glob(os.path.join(patient__folder, "img", "*.png"))
                img_files = sorted(img_files, key=lambda x: int(os.path.basename(x).split('.')[0]))


                mask_files = glob.glob(os.path.join(patient__folder, "mask_color", "*.png"))
                mask_files = sorted(mask_files, key=lambda x: int(os.path.basename(x).split('_')[0]))

                train_images, test_images, train_masks, test_masks = train_test_split(
                                img_files, mask_files, test_size=0.1, random_state=42)

                train_images, val_images, train_masks, val_masks = train_test_split(
                                train_images, train_masks, test_size=0.2, random_state=42)

                all_train_images.extend(train_images)
                all_test_images.extend(test_images)
                all_val_images.extend(val_images)
                all_train_masks.extend(train_masks)
                all_test_masks.extend(test_masks)
                all_val_masks.extend(val_masks)

    return all_train_images, all_test_images, all_val_images, all_train_masks, all_test_masks, all_val_masks

class RandomAffine:
    def __init__(self, max_scale, max_xt, max_yt, max_angle, max_shear):
        self.max_scale = max_scale
        self.max_xt = max_xt
        self.max_yt = max_yt
        self.max_angle = max_angle
        self.max_shear = max_shear

    def __call__(self, sample):
        image, mask = sample['image'], sample['labels']
        h, w = image.shape[:2]
        scale = 1.0 + random.uniform(-self.max_scale, self.max_scale)
        xt = w * random.uniform(-self.max_xt, self.max_xt)
        yt = h * random.uniform(-self.max_yt, self.max_yt)
        angle = random.uniform(-self.max_angle, self.max_angle)
        shear = random.uniform(-self.max_shear, self.max_shear)

        im_pil = p_im.fromarray(image)
        if image.ndim == 3 and image.shape[2] == 3:
            im_out = transforms.functional.affine(im_pil,
                angle, (xt, yt), scale, shear, p_im.BILINEAR, fill=0)
        elif image.ndim == 2:
            im_out = transforms.functional.affine(im_pil,
                angle, (xt, yt), scale, shear, p_im.BILINEAR, fill=0)

        mask_pil = p_im.fromarray(mask.astype(np.uint8))
        m_out = transforms.functional.affine(mask_pil,
            angle, (xt, yt), scale, shear, p_im.NEAREST, fill=0)

        return {'image': np.array(im_out), 'labels': np.array(m_out)}

class HorizontalFlip(object):
    """Flip image horizontally.

    Args:
        <none>
    """

    def __init__(self):
        pass

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        image_out = cv2.flip(batch_images, 1)
        label_out = cv2.flip(batch_labels, 1)

        return {'image': np.array(image_out), 'labels': np.array(label_out)}

class RandomRescale(object):
    """Rescale the image in a sample to a given size.

    Args:
        scale_limit (float or tuple): Scaling factor range.
        interpolation (int): Interpolation algorithm.
    """
    def __init__(self, scale_limit=(-0.1, 0.1), interpolation=cv2.INTER_LINEAR):
        if isinstance(scale_limit, (int, float)):
            scale_limit = (-scale_limit, scale_limit)
        assert len(scale_limit) == 2

        self.scale_limit = scale_limit
        self.interpolation = interpolation

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        low, high = self.scale_limit
        scale = 1.0 + random.uniform(low, high)

        image_out = transforms.functional.affine(
                p_im.fromarray(batch_images),
                0, (0, 0), scale, 0, self.interpolation, 0
            )

        labels_out =  batch_labels

        return {'image': np.array(image_out), 'labels': np.array(labels_out)}

class RandomShift(object):
    """Randomly shift/translate the image in x and y axes.

    Args:
        max_xt (float): Maximum x-axis translation.
        max_yt (float): Maximum y-axis translation.
    """

    def __init__(self, max_xt, max_yt):
        self.max_xt = max_xt
        self.max_yt = max_yt

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        x = random.uniform(-self.max_xt, self.max_xt)
        y = random.uniform(-self.max_yt, self.max_yt)

        image_out = transforms.functional.affine(p_im.fromarray(batch_images),
            0, (x,y), 1.0, 0, p_im.BILINEAR, 0)

        return {'image': np.array(image_out), 'labels': sample['labels']}

class CentreCrop(object):
    """Centre crop the image in a sample.

    Args:
        output_size (tuple or int): Desired output size. If int, square crop
            is made.
    """

    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            assert len(output_size) == 2
            self.output_size = output_size

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        h, w, c = batch_images.shape

        new_h, new_w = self.output_size

        y1 = (h - new_h) // 2
        y2 = y1 + new_h
        x1 = (w - new_w) // 2
        x2 = x1 + new_w

        image_cropped = batch_images[y1:y2, x1:x2]
        labels_cropped = batch_labels[y1:y2, x1:x2]

        return {'image': np.array(image_cropped), 'labels': np.array(labels_cropped)}


class Normalise(object):
    """Scale and shift image by mu and sigma """

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        batch_images = batch_images.astype(np.float32)
        batch_images /= 255.0

        return {'image': batch_images, 'labels': batch_labels}

class Resize(object):
    """Scale and shift image by mu and sigma """
    def __init__(self, size):
        self.size = size

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        image_resized = cv2.resize(batch_images, self.size, interpolation=cv2.INTER_NEAREST_EXACT)
        labels_resized = cv2.resize(batch_labels, self.size, interpolation=cv2.INTER_NEAREST_EXACT)


        return {'image': image_resized, 'labels': labels_resized}


class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""
    def __init__(self, label_scale):
        self.label_scale = label_scale

    def __call__(self, sample):
        batch_images, batch_labels = sample['image'], sample['labels']

        image_t = torch.from_numpy(batch_images)
        labels_t = torch.from_numpy(np.round(self.label_scale * batch_labels, decimals=1))

        return {'image': image_t,
                'labels': labels_t}

def get_transforms(*, data):

    assert data in ('train', 'valid')

    if data == 'train':
        return Compose([
    RandomAffine(max_scale=0.0, max_xt=0.0, max_yt=0.0, max_angle=2, max_shear=0.0),
    RandomRescale(scale_limit = 0.01 ,interpolation= cv2.INTER_NEAREST),
    CentreCrop(output_size=(500, 500)),
    #RandomShift(max_xt=0.01, max_yt=0.01),
    HorizontalFlip(),
    Resize((240,240)),
    Normalise(),
    ToTensor(label_scale=1.0)
        ])

    elif data == 'valid':
        return Compose([
    Resize((224,224)),
    Normalise(),
    ToTensor(label_scale=1.0)
        ])

class CustomDataset(Dataset):
    def __init__(self, image_files, mask_files, input_size, augmentation_transforms=None):
        self.image_files = image_files
        self.mask_files = mask_files
        self.input_size = input_size
        self.augmentation_transforms = augmentation_transforms

        self.BENIGN_CLASSES = [29, 225]
        self.MALIGNANT_CLASSES = [76, 149, 178]
        self.UNLABELED_CLASSES = [75]
        self.ARTEFACT_CLASSES = [128]

        self.BENIGN_LABEL = 1
        self.MALIGNANT_LABEL = 2
        self.UNLABELED_LABEL = 3
        self.ARTEFACT_LABEL = 4

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        image_path = self.image_files[idx]
        mask_path = self.mask_files[idx]

        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, self.input_size)

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.resize(mask, self.input_size)

        mask = self.create_multiclass_mask(mask)

        if self.augmentation_transforms:
            sample = {'image': image, 'labels': mask}
            augmented_sample = self.augmentation_transforms(sample)
            image = augmented_sample['image']
            mask = augmented_sample['labels']

        image = np.transpose(image, (2, 0, 1))

        return image, mask

    def create_multiclass_mask(self, mask):
        final_mask = np.zeros(mask.shape, dtype=np.uint8)

        benign_mask = np.isin(mask, self.BENIGN_CLASSES)
        final_mask[benign_mask] = self.BENIGN_LABEL

        malignant_mask = np.isin(mask, self.MALIGNANT_CLASSES)
        final_mask[malignant_mask] = self.MALIGNANT_LABEL

        unlabeled_mask = np.isin(mask, self.UNLABELED_CLASSES)
        final_mask[unlabeled_mask] = self.UNLABELED_LABEL

        artefact_mask = np.isin(mask, self.ARTEFACT_CLASSES)
        final_mask[artefact_mask] = self.ARTEFACT_LABEL

        return final_mask

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

class UNet_apex(nn.Module):
    def __init__(self, in_channels= 3, n_classes= 4, depth= 5,
        n_base_filters= 8, n_segmentation_layers= 4,
        padding= True, norm_type= 'instance', activation_type= 'relu', up_mode= 'upsample',
        dropout= 0.3):

        super(UNet_apex, self).__init__()
        assert up_mode in ('upconv', 'upsample')
        self.padding = padding
        self.depth = depth
        n_level_filters = in_channels

        self.down_conv = nn.ModuleList()
        self.down_context = nn.ModuleList()
        level_filters = list()

        stride=1
        for i_level in range(depth):
            prev_channels = n_level_filters
            n_level_filters = (2**i_level) * n_base_filters
            level_filters.append(n_level_filters)

            if i_level: #current_layer is inputs:
                stride=2

            self.down_conv.append(UNetConvBlock(prev_channels, n_level_filters,
                padding, norm_type, activation_type, stride=stride))

            self.down_context.append(UNetContextBlock(n_level_filters, dropout,
                padding, norm_type, activation_type))

        self.up_conv = nn.ModuleList()
        self.localization_layers = nn.ModuleList()
        self.segmentation_layers = nn.ModuleList()
        for i_level in range(depth - 1):
            nf_in = level_filters[-i_level-1]
            nf_out = level_filters[-i_level-2]

            self.up_conv.append(
                UNetUpBlock(nf_in, nf_out, up_mode,
                    padding, norm_type, activation_type))

            self.localization_layers.append(
                UNetLocalizationBlock(nf_in, nf_out,
                    padding, norm_type, activation_type)
            )

        self.segmentation_layers = nn.ModuleList()
        for i_level in range(n_segmentation_layers):
            nf_in = level_filters[n_segmentation_layers-i_level-1]
            self.segmentation_layers.append(
                nn.Conv2d(nf_in, n_classes, kernel_size=1)
            )



    def forward(self, x):
        bridge_outputs = []
        depth = len(self.down_conv)
        n_segmentation_layers = len(self.segmentation_layers)
        seg_offset = depth-n_segmentation_layers-1

        for i_level in range(depth):
            #1) Apply conv block
            conv_out = self.down_conv[i_level](x)
            #2) Apply context block to conv ouput
            context = self.down_context[i_level](conv_out)
            #3) Add conv output to context output
            x = torch.add(conv_out, context)
            if i_level != depth-1:
                bridge_outputs.append(x)

        for i_level in range(depth - 1):
            #seg layers are d-n_seg
            x = self.up_conv[i_level](x, bridge_outputs[-i_level-1])
            x = self.localization_layers[i_level](x)
            if i_level >= seg_offset: #is seg layer
                seg_layer = i_level - seg_offset
                if seg_layer == 0: #is first seg layer
                    seg_out = self.segmentation_layers[seg_layer](x)

                else: #is 2nd-ary seg layer
                    seg_out = torch.add(seg_out, self.segmentation_layers[seg_layer](x))

                if seg_layer < n_segmentation_layers-1: #isn't last seg layer
                    seg_out = F.interpolate(seg_out, scale_factor=2, mode='bilinear')

        seg_out_softmax = F.softmax(seg_out, dim=1)

        return seg_out


class UNetConvBlock(nn.Module):
    def __init__(self, in_size, out_size, padding, norm_type, activation_type,
        kernel_size = 3, stride=1):
        super(UNetConvBlock, self).__init__()
        block = []

        block.append(nn.Conv2d(in_size, out_size, kernel_size=kernel_size,
                               padding=int(padding), stride=stride))
        if norm_type == 'batch':
            block.append(nn.BatchNorm2d(out_size))
        elif norm_type == 'instance':
            block.append(nn.InstanceNorm2d(out_size))

        if activation_type == 'relu':
            block.append(nn.ReLU())
        elif activation_type == 'leaky':
            block.append(nn.LeakyReLU())

        self.block = nn.Sequential(*block)

    def forward(self, x):
        out = self.block(x)
        return out

class UNetContextBlock(nn.Module):
    def __init__(self, n_level_filters, dropout, padding, norm_type, activation_type):
        super(UNetContextBlock, self).__init__()

        self.conv_block1 = UNetConvBlock(n_level_filters, n_level_filters,
            padding, norm_type, activation_type)
        self.dropout = nn.Dropout2d(dropout)
        self.conv_block2 = UNetConvBlock(n_level_filters, n_level_filters,
            padding, norm_type, activation_type)

    def forward(self, x):
        c1_out = self.conv_block1(x)
        d = self.dropout(c1_out)
        return self.conv_block2(d)


class Upsample(nn.Module):
    def __init__(self,  scale_factor, mode):
        super(Upsample, self).__init__()
        self.scale_factor = scale_factor
        self.mode = mode
    def forward(self, x):
        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)

class UNetUpBlock(nn.Module):
    def __init__(self, in_size, out_size, up_mode, padding, norm_type, activation_type):
        super(UNetUpBlock, self).__init__()
        if up_mode == 'upconv':
            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,
                                         stride=2)
        elif up_mode == 'upsample':
            self.up = Upsample(scale_factor=2, mode='bilinear')

        self.conv_block = UNetConvBlock(in_size, out_size,
            padding, norm_type, activation_type)

    def center_crop(self, layer, target_size):
        _, _, layer_y, layer_x = layer.size()
        diff_y = (layer_y - target_size[0]) // 2
        diff_x = (layer_x - target_size[1]) // 2
        return layer[:, :,
            diff_y:(diff_y + target_size[0]),
            diff_x:(diff_x + target_size[1])]

    def forward(self, x, bridge):
        up = self.up(x)
        up = self.conv_block(up)
        crop1 = self.center_crop(bridge, up.shape[2:])
        out = torch.cat([crop1, up], 1)
        return out

class UNetLocalizationBlock(nn.Module):
    def __init__(self, in_size, out_size, padding, norm_type, activation_type):
        super(UNetLocalizationBlock, self).__init__()
        self.block = nn.Sequential(
            UNetConvBlock(in_size, out_size, padding, norm_type, activation_type),
            UNetConvBlock(out_size, out_size, 0, norm_type, activation_type,
                kernel_size = 1)
        )

    def forward(self, x):
        out = self.block(x)
        return out

model = UNet_apex( )

for i, seg_layer in enumerate(model.segmentation_layers):
    print(f"Segmentation Layer {i + 1}: Output Channels = {seg_layer.out_channels}")

print(summary(model, input_size = (4, 3, 128, 128), device="cpu"))

class DiceLoss(nn.Module):
    def __init__(self, n_classes):
        super(DiceLoss, self).__init__()
        self.n_classes = n_classes

    def _one_hot_encoder(self, input_tensor):
        tensor_list = []
        for i in range(self.n_classes):
            temp_prob = input_tensor == i * torch.ones_like(input_tensor)
            tensor_list.append(temp_prob)
        output_tensor = torch.cat(tensor_list, dim=1)
        return output_tensor.float()

    def _dice_loss(self, score, target):
        target = target.float()
        smooth = 1e-5
        intersect = torch.sum(score * target)
        y_sum = torch.sum(target * target)
        z_sum = torch.sum(score * score)
        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)
        loss = 1 - loss
        return loss

    def forward(self, inputs, target, weight=None, softmax=False):
        if softmax:
            inputs = torch.softmax(inputs, dim=1)
        target = self._one_hot_encoder(target)
        if weight is None:
            weight = [1] * self.n_classes
        assert inputs.size() == target.size(), 'predict & target shape do not match'
        class_wise_dice = []
        loss = 0.0
        for i in range(0, self.n_classes):
            dice = self._dice_loss(inputs[:, i], target[:, i])
            class_wise_dice.append(1.0 - dice.item())
            loss += dice * weight[i]
        return loss / self.n_classes

def calculate_metric_percase(pred, gt):
    pred[pred > 0] = 1
    gt[gt > 0] = 1
    if pred.sum() > 0:
        dice = metric.binary.dc(pred, gt)
        hd95 = metric.binary.jc(pred, gt)
        return dice, hd95
    else:
        return 0, 0

def test_single_volume(image, label, model, classes, patch_size=[256, 256]):
    image, label = image.cpu().detach().numpy(), label.cpu().detach().numpy()
    prediction = np.zeros_like(label)

    for ch in range(image.shape[0]):
        slice = image[ch, :, :]
        input = torch.from_numpy(slice).unsqueeze(0).float().cpu()
        model.eval()

        with torch.no_grad():
            output = model(input)
            if len(output) > 1:
                output = output[0]
            out = torch.argmax(torch.softmax(output, dim=1), dim=1).squeeze(0)
            out = out.cpu().detach().numpy()
            prediction[ch, :, :] = out

    metric_list = []

    for i in range(1, classes):
        metric_list.append(calculate_metric_percase(prediction == i, label == i))

    return metric_list

def pretrain(model, optimizer, trainloader, valloader, model_path, N_EPOCHS, dice_loss, lr, num_classes):
    iters = 0
    total_iters = len(trainloader) * N_EPOCHS
    checkpoints = []
    best_performance = 0.0
    device = next(model.parameters()).device
    weights = [0.1167, 0.0763, 0.1702, 0.6371]

    train_losses = []
    val_dice_metrics = []

    metrics_file = os.path.join(model_path, 'metrics.csv')
    with open(metrics_file, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['Epoch', 'Train_Loss', 'Val_Dice_Metric', 'Val_IoU_Metric', 'Learning Rate'])

        for epoch in tqdm(range(1, N_EPOCHS + 1), ncols=70):
            model.train()
            train_loss_sum = 0.0

            for batch_images, batch_masks in tqdm(trainloader):
                volume_batch, label_batch = batch_images.to(device), batch_masks.to(device)

                outputs = model(volume_batch)

                outputs_soft = torch.softmax(outputs, dim=1)
                loss_dice1 = dice_loss(outputs_soft, label_batch.unsqueeze(1), weight = weights)

                optimizer.zero_grad()
                loss_dice1.backward()
                optimizer.step()

                iters += 1
                lr_ = lr * (1 - iters / total_iters) ** 0.9
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr_

                train_loss_sum += loss_dice1.item()

            train_loss_avg = train_loss_sum / len(trainloader)
            train_losses.append(train_loss_avg)

            model.eval()
            metric_list = 0.0

            for batch_images, batch_masks in tqdm(valloader):
                images, labels = batch_images.to(device), batch_masks.to(device)

                metric_i = test_single_volume(images, labels, model, classes=num_classes)
                metric_list += np.array(metric_i)
                metric_list = metric_list / len(valloader)

            performance = np.mean(metric_list, axis=0)[0]
            mean_iou = np.mean(metric_list, axis=0)[1]

            val_dice_metrics.append(performance)

            print(f'Epoch {epoch}/{N_EPOCHS}')
            print(f'Train Loss: {train_loss_avg:.4f}')
            print(f'Val Dice Metric: {performance:.4f} Val IoU Metric: {mean_iou:.4f}')

            writer.writerow([epoch, train_loss_avg, performance, mean_iou, lr_])

            if performance > best_performance:
                if best_performance != 0:
                    os.remove(os.path.join(model_path, save_model_name))

                best_performance = performance
                save_model_name = f'epoch_{epoch}_metric_{round(best_performance, 4)}.pth'

                torch.save(model.state_dict({
                'epoch' : epoch,
                'model_state_dict' : model.state_dict(),
                'optimizer_state_dict' : optimizer.state_dict()}), os.path.join(model_path, save_model_name))
                checkpoints = deepcopy(model)

            plt.figure(figsize=(10, 5))
            plt.plot(range(1, epoch + 1), train_losses, label='Train Loss')
            plt.plot(range(1, epoch + 1), val_dice_metrics, label='Val Dice Metric')
            plt.xlabel('Epoch')
            plt.ylabel('Value')
            plt.legend()
            plt.title('Train Loss and Val Dice Metric over Epochs')
            plt.grid(True)

            plot_save_path = os.path.join(model_path, f'epoch_{N_EPOCHS}_plot.png')
            plt.savefig(plot_save_path)
            plt.close()

    return checkpoints

dataset_root = r'F:\train_dataset'
result_root = r'F:\save'

train_images, test_images, val_images, train_masks, test_masks, val_masks = get_all_patient_images(dataset_root)

# train_images = train_images[:10]
# test_images = test_images[:10]
# val_images = val_images[:10]
# train_masks = train_masks[:10]
# test_masks = test_masks[:10]
# val_masks = val_masks[:10]

print(f"Train set size: {len(train_images)} samples")
print(f"Validation set size: {len(val_images)} samples")
print(f"Test set size: {len(test_images)} samples")

train_dataset = CustomDataset(train_images, train_masks, (512,512),augmentation_transforms = get_transforms(data='train'))
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

val_dataset = CustomDataset(val_images, val_masks, (512,512),augmentation_transforms = get_transforms(data='valid'))
val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)

test_dataset = CustomDataset(test_images, test_masks, (512,512),augmentation_transforms = get_transforms(data='valid'))
test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)

for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):
    if (batch_idx + 1) % 1 == 0:
        print("Train Batch", batch_idx + 1)

for batch_idx, (batch_images, batch_masks) in enumerate(test_dataloader):
    if (batch_idx + 1) % 1 == 0:
        print("Test Batch", batch_idx + 1)
        print(batch_images.shape)
for batch_idx, (batch_images, batch_masks) in enumerate(val_dataloader):
    if (batch_idx + 1) % 100 == 0:
        print("Val Batch", batch_idx + 1)

def plot_image_and_mask(image, mask, index):
    image = image.numpy().transpose(1, 2, 0)
    mask = mask.numpy()
    print(np.unique(mask))
    fig, ax = plt.subplots(1, 2, figsize=(12, 6))

    ax[0].imshow(image)
    ax[0].set_title(f'Image {index}')
    ax[0].axis('off')

    ax[1].imshow(mask, cmap='gray')
    ax[1].set_title(f'Mask {index}')
    ax[1].axis('off')

    plt.show()

for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):
    if (batch_idx + 1) % 1 == 0:

        plot_image_and_mask(batch_images[0], batch_masks[0], index=1)
        break

classes = ["Benign", "Malignant", "Unlabeled", "Artefact"]
class_counts = {class_name: 0 for class_name in classes}

for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):
    # print("Batch", batch_idx + 1)

    for image, mask in zip(batch_images, batch_masks):
        image = image.numpy().transpose(1, 2, 0)
        mask = mask.numpy().transpose(1, 2, 0)

        plt.figure(figsize=(15, 10))

        plt.subplot(2, 4, 1)
        plt.imshow(image)
        plt.title("Original Image")

        for class_idx, class_name in enumerate(classes):
            plt.subplot(2, 4, class_idx + 2)
            class_mask = mask[:, :, class_idx]
            plt.imshow(class_mask, cmap='gray', vmin=0, vmax=1)
            plt.title(f"Class {class_name} Mask")
            unique_values = np.unique(class_mask)
            print(f"Unique values in {class_name} mask: {unique_values}")


        plt.tight_layout()
        plt.show()

"""classes = ["Benign", "Malignant", "Unlabeled", "Artefact"]
class_counts = {class_name: 0 for class_name in classes}

for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):
    # print("Batch", batch_idx + 1)

    for image, mask in zip(batch_images, batch_masks):
        image = image.numpy().transpose(1, 2, 0)  
        mask = mask.numpy().transpose(1, 2, 0)

        plt.figure(figsize=(15, 10))

        plt.subplot(2, 4, 1)
        plt.imshow(image)
        plt.title("Original Image")

        for class_idx, class_name in enumerate(classes):
            plt.subplot(2, 4, class_idx + 2)
            class_mask = mask[:, :, class_idx]
            plt.imshow(class_mask, cmap='gray', vmin=0, vmax=1)
            plt.title(f"Class {class_name} Mask")
            unique_values = np.unique(class_mask)
            print(f"Unique values in {class_name} mask: {unique_values}")


        plt.tight_layout()
        plt.show()



"""

NUM_CLASSES = 4
N_EPOCHS = 50

MODEL_NAME = f'UNet_epochs{N_EPOCHS}'
MAX_LR = 1e-3

class_weights = torch.tensor([0.1167, 0.0763, 0.1699, 0.6371], dtype=torch.float32)

model = UNet_apex()

dice_loss = DiceLoss(n_classes=4)

optimizer = optim.Adam(model.parameters(), lr=MAX_LR,
                       eps=1e-08, weight_decay=1e-3, amsgrad=True)

scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,
                                               mode='min', factor= 0.1,
                                               patience= 10, verbose=False,
                                               threshold=0.0001, threshold_mode='rel',
                                               cooldown=0, min_lr=0, eps=1e-08)

device = get_default_device()
print(device)

train_loader = train_dataloader
val_loader = val_dataloader

checkpoints = pretrain(model, optimizer, train_dataloader, val_dataloader, N_EPOCHS = N_EPOCHS, model_path = result_root,
                       dice_loss = dice_loss, lr = MAX_LR, num_classes = NUM_CLASSES )

def inference(model, dataloader, device, threshold=0.5):
    model.eval()
    predictions = []

    with torch.no_grad():
        for inputs, _ in tqdm(dataloader):
            inputs = inputs.to(device)

            outputs = model(inputs)

            predictions_batch = (torch.sigmoid(outputs) > threshold).cpu().numpy()
            predictions.append(predictions_batch)

    return np.concatenate(predictions, axis=0)

# model.load_state_dict(torch.load(r'F:\save\epoch_40_metric_0.002.pth'))
# model_t = model.to(device)
checkpoint = torch.load(r'F:\save\epoch_40_metric_0.002.pth')  # Load the checkpoint
model.load_state_dict(checkpoint['model_state_dict'])          # Extract and load the model's state dictionary

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_t = model.to(device)

predictions = inference(model_t, test_dataloader, device)

test_loss, test_dice_coefficient = evaluate_model(model_t, test_dataloader, criterion, num_classes=NUM_CLASSES, device=device, threshold=0.5)
print(f'Test Loss: {test_loss:.5f}, Test Dice Coefficient: {test_dice_coefficient:.5f}')

for batch_images, batch_masks in test_dataloader:
    print("Shape of batch_images:", batch_images.shape)
    print("Shape of batch_masks:", batch_masks.shape)

    # Ensure correct shape for batch_images
    batch_images_np = batch_images.numpy().transpose(0, 2, 3, 1)

    # Add channel dimension to batch_masks
    batch_masks_np = batch_masks.unsqueeze(1).numpy().transpose(0, 2, 3, 1)

    for idx in range(len(batch_images)):
        plt.figure(figsize=(15, 5))

        for class_idx, class_name in enumerate(classes):
            plt.subplot(2, len(classes), class_idx + 1)
            plt.imshow(batch_masks_np[idx, :, :, 0], cmap='gray', vmin=0, vmax=1)
            plt.title(f"Original {class_name} Mask")

        predicted_masks = predict_masks(model, batch_images)

        for class_idx, class_name in enumerate(classes):
            plt.subplot(2, len(classes), len(classes) + class_idx + 1)
            plt.imshow(predicted_masks[idx, :, :, class_idx], cmap='gray', vmin=0, vmax=1)
            plt.title(f"Predicted {class_name} Mask")

        plt.tight_layout()
        plt.show()
        break

print("Shape of batch_masks:", batch_masks.shape)

